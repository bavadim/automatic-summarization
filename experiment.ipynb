{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AAWVFWAYmRwR",
    "outputId": "f23aa268-60f9-497e-bf0e-05f6d60ab9b9"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "#%load_ext tensorboard\n",
    "#!pip3 -q install -U tensorflow==2.1.0 tensorflow-gpu==2.1.0 tensorflow-datasets==2.1.0 tensorflow-text==2.1.1 tensorflow-hub==0.7.0 nltk sklearn transformers tensorflow-addons \n",
    "!pip3 -q install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using datasets to cache modules.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow_datasets as tfds\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = 'datasets'\n",
    "\n",
    "\n",
    "class BaseSummarizer(object):\n",
    "    ROUND_DIGITS = 5\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
    "        normalize = tf.math.l2_normalize(vec, 1)\n",
    "        cosine = tf.linalg.matmul(normalize, normalize, transpose_b=True)\n",
    "        rounded = tf.math.round(cosine * 10 ** BaseSummarizer.ROUND_DIGITS) / 10 ** BaseSummarizer.ROUND_DIGITS\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def __ranks__(sent_sim_mat: tf.Tensor) -> tf.Tensor:\n",
    "        eig_val, eig_vec = tf.linalg.eigh(sent_sim_mat)\n",
    "        best_vector_idx = tf.math.argmax(eig_val)\n",
    "        return eig_vec[best_vector_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def __z_score__(vec: tf.Tensor) -> tf.Tensor:\n",
    "        return (vec - tf.math.reduce_min(vec)) / (tf.math.reduce_max(vec) - tf.math.reduce_min(vec))\n",
    "\n",
    "    def bleu(self, references: List[List[str]], texts: List[str]):\n",
    "        score = 0.\n",
    "        smoothie = SmoothingFunction().method1\n",
    "\n",
    "        for refs, txt in zip(references, texts):\n",
    "            hyp = self.the_most_important(txt, k=1)[0]\n",
    "            score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
    "\n",
    "        score /= len(references)\n",
    "        return score\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "        sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
    "        ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "        return list(zip(sents, ranks.numpy()))\n",
    "\n",
    "    def the_most_important(self, text, k=1):\n",
    "        return [ p[0] for p in sorted(self.scored_sentences(text), key=lambda p: p[1], reverse=True)[:k] ]\n",
    "\n",
    "\n",
    "class USETextRank(BaseSummarizer):\n",
    "    __embed__ = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return self.__embed__(sentences)\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "class TFIDFTextRank(BaseSummarizer):\n",
    "    __vectorizer__ = TfidfVectorizer()\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return tf.constant(self.__vectorizer__.fit_transform(sentences).todense())\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "summarizerUSE = USETextRank()\n",
    "summarizerTFIDF = TFIDFTextRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_BOHGvJy44R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MAX_SEQ_LENGTH = 256\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_subnet = hub.KerasLayer(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", \n",
    "                      signature=\"tokens\", output_key=\"pooled_output\", trainable=True)\n",
    "\n",
    "def embedding4pair(s1: str, s2: str) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    r = tokenizer.encode_plus(\n",
    "          pad_to_max_length='right',\n",
    "          text=s1,\n",
    "          text_pair=s2,\n",
    "          max_length=MAX_SEQ_LENGTH)\n",
    "    \n",
    "    return tf.constant(r['input_ids']), tf.constant(r['attention_mask']), tf.constant(r['token_type_ids'])\n",
    "\n",
    "\n",
    "def create_ruler() -> tf.keras.Model:\n",
    "    i_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n",
    "    i_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=tf.int32)\n",
    "    i_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_inputs = {\"input_ids\": i_id, \"input_mask\": i_mask, \"segment_ids\": i_segment}\n",
    "  \n",
    "    embedding = bert_subnet(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, input_shape=(768,), activation='relu')(embedding)\n",
    "    d = tf.keras.layers.Dense(1, input_shape=(256,))(dense)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=bert_inputs, outputs=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "G-QtIs37IvMg",
    "outputId": "29153f72-1eb2-448d-ab22-d18cb176a694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation_matched, from datasets/multi_nli/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from datasets/multi_nli/plain_text/1.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "nli_validation = tfds.load(name=\"multi_nli\", split='validation_matched', data_dir='datasets')\n",
    "nli_train = tfds.load(name=\"multi_nli\", split='train', data_dir='datasets')\n",
    "\n",
    "def process_dataset(ds):\n",
    "    premises = []\n",
    "    hypothesis = []\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "    for x in ds:\n",
    "        p = x['premise'].numpy().decode('utf8')\n",
    "        l = x['label'].numpy()\n",
    "        h = x['hypothesis'].numpy().decode('utf8')\n",
    "        r = tokenizer.encode_plus(\n",
    "          pad_to_max_length='right',\n",
    "          text=p,\n",
    "          text_pair=h,\n",
    "          max_length=MAX_SEQ_LENGTH)\n",
    "  \n",
    "        input_ids.append(r['input_ids'])\n",
    "        input_mask.append(r['attention_mask'])\n",
    "        segment_ids.append(r['token_type_ids'])\n",
    "        if l == 1:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "        premises.append(p)\n",
    "        hypothesis.append(h)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    input_mask = np.array(input_mask, dtype=np.int32)\n",
    "    segment_ids = np.array(segment_ids, dtype=np.int32)\n",
    "\n",
    "    labels = np.array(labels, dtype=np.float16)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((input_ids, input_mask, segment_ids), labels))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "ds_nli_train = process_dataset(nli_train)\n",
    "ds_nli_valid = process_dataset(nli_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "gy8-FvD1eGof",
    "outputId": "6529029a-5559-44f8-8469-ceeedfcca742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 767 steps, validate for 20 steps\n",
      "Epoch 1/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2209 - mean_squared_error: 0.2209 \n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4154s 5s/step - loss: 0.2209 - mean_squared_error: 0.2209 - val_loss: 0.2413 - val_mean_squared_error: 0.2413\n",
      "Epoch 2/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2120 - mean_squared_error: 0.2120 \n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4156s 5s/step - loss: 0.2120 - mean_squared_error: 0.2120 - val_loss: 0.1985 - val_mean_squared_error: 0.1985\n",
      "Epoch 3/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2091 - mean_squared_error: 0.2091 \n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4157s 5s/step - loss: 0.2091 - mean_squared_error: 0.2091 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 4/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2049 - mean_squared_error: 0.2049 \n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.2049 - mean_squared_error: 0.2049 - val_loss: 0.1929 - val_mean_squared_error: 0.1929\n",
      "Epoch 5/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2022 - mean_squared_error: 0.2022 \n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4158s 5s/step - loss: 0.2022 - mean_squared_error: 0.2022 - val_loss: 0.1920 - val_mean_squared_error: 0.1920\n",
      "Epoch 6/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2020 - mean_squared_error: 0.2020 \n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.2020 - mean_squared_error: 0.2020 - val_loss: 0.1916 - val_mean_squared_error: 0.1916\n",
      "Epoch 7/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2009 - mean_squared_error: 0.2009 \n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4156s 5s/step - loss: 0.2009 - mean_squared_error: 0.2009 - val_loss: 0.1881 - val_mean_squared_error: 0.1881\n",
      "Epoch 8/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1996 - mean_squared_error: 0.1996 \n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.1996 - mean_squared_error: 0.1996 - val_loss: 0.1979 - val_mean_squared_error: 0.1979\n",
      "Epoch 9/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1970 - mean_squared_error: 0.1970 \n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4153s 5s/step - loss: 0.1970 - mean_squared_error: 0.1970 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 11/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1966 - mean_squared_error: 0.1966 "
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "model = create_ruler()\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                            save_weights_only=True,\n",
    "                                            verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    ds_nli_train.shuffle(32868).batch(512), \n",
    "    validation_data=ds_nli_valid.batch(512), \n",
    "    callbacks=[es_cb, cp_cb], \n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRRBV9iszaMs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XQdA0YigClMK",
    "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('cnn_dailymail.csv')\n",
    "references = df_test['highlights'].map(lambda x: np.array(x.split('\\n'), dtype=str)).values\n",
    "txts = df_test['article'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdAW-Hrmr_ix"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[729,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert/encoder/layer_0/attention/self/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_function_4889786]\n\nFunction call stack:\nfunction\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6e7b5cc1d271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msummarizerBERT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTTextRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizerBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a3b8b5999dbe>\u001b[0m in \u001b[0;36mbleu\u001b[0;34m(self, references, texts)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mhyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthe_most_important\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a3b8b5999dbe>\u001b[0m in \u001b[0;36mthe_most_important\u001b[0;34m(self, text, k)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthe_most_important\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscored_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a3b8b5999dbe>\u001b[0m in \u001b[0;36mscored_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msim_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sim_mat__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__embeddings__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__z_score__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ranks__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6e7b5cc1d271>\u001b[0m in \u001b[0;36m__sim_mat__\u001b[0;34m(self, vec)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict_on_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_run_tf_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_v2_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandalone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     if (self._distribution_strategy and\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(model, x, standalone)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_learning_phase_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_on_batch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[729,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_1/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert/encoder/layer_0/attention/self/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_function_4889786]\n\nFunction call stack:\nfunction\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = tf.keras.models.load_model('simple', compile=False )\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "r = embedding4pair('олооло ава ', 'sdsdasds')\n",
    "        \n",
    "    \n",
    "class BERTTextRank(BaseSummarizer):\n",
    "    \n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return tf.constant(sentences, dtype=tf.string)\n",
    "    \n",
    "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
    "        vecs = vec.numpy()\n",
    "        \n",
    "        res = []\n",
    "        i = []\n",
    "        a = []\n",
    "        m = []\n",
    "         \n",
    "        for v1 in vecs:\n",
    "            for v2 in vecs:\n",
    "                r = embedding4pair(v1.decode('utf8'), v2.decode('utf8'))\n",
    "                i.append(r[0])\n",
    "                a.append(r[1])\n",
    "                m.append(r[2])\n",
    "        p = model.predict_on_batch((i, a, m)).numpy()\n",
    "        res = p.reshape((len(vecs), len(vecs)))\n",
    "                \n",
    "        return res\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "    \n",
    "summarizerBERT = BERTTextRank()\n",
    "print('bert', summarizerBERT.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = embedding4pair('Чувак с автоматом гоняет обезьяну', 'обезьяна бегает')\n",
    "r[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BekUXV9JADn-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use 0.061298139955609794\n"
     ]
    }
   ],
   "source": [
    "print('use', summarizerUSE.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7wNS9tOPr9M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0.06028948961872044\n"
     ]
    }
   ],
   "source": [
    "print('tfidf', summarizerTFIDF.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTFTextRank(BaseSummarizer):\n",
    "      #__tokenizer__ = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      #__embed__ = hub.Module(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", trainable=False)\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "\n",
    "        \n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
