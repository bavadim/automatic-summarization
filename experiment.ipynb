{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AAWVFWAYmRwR",
    "outputId": "f23aa268-60f9-497e-bf0e-05f6d60ab9b9"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "#%load_ext tensorboard\n",
    "!pip3 -q install tensorflow==2.1.0 tensorflow-gpu==2.1.0 tensorflow-datasets==2.1.0 tensorflow-text==2.1.1 tensorflow-hub==0.7.0 nltk sklearn transformers tensorflow-addons\n",
    "#!ulimit -n 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class BaseSummarizer(object):\n",
    "    ROUND_DIGITS = 5\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
    "        normalize = tf.math.l2_normalize(vec, 1)\n",
    "        cosine = tf.linalg.matmul(normalize, normalize, transpose_b=True)\n",
    "        rounded = tf.math.round(cosine * 10 ** BaseSummarizer.ROUND_DIGITS) / 10 ** BaseSummarizer.ROUND_DIGITS\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def __ranks__(sent_sim_mat: tf.Tensor) -> tf.Tensor:\n",
    "        eig_val, eig_vec = tf.linalg.eigh(sent_sim_mat)\n",
    "        best_vector_idx = tf.math.argmax(eig_val)\n",
    "        return eig_vec[best_vector_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def __z_score__(vec: tf.Tensor) -> tf.Tensor:\n",
    "        return (vec - tf.math.reduce_min(vec)) / (tf.math.reduce_max(vec) - tf.math.reduce_min(vec))\n",
    "\n",
    "    def bleu(self, references: List[List[str]], texts: List[str]):\n",
    "        score = 0.\n",
    "        smoothie = SmoothingFunction().method1\n",
    "\n",
    "        for refs, txt in zip(references, texts):\n",
    "            hyp = self.the_most_important(txt, k=1)[0]\n",
    "            score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
    "\n",
    "        score /= len(references)\n",
    "        return score\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "        sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
    "        ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "        return list(zip(sents, ranks.numpy()))\n",
    "\n",
    "    def the_most_important(self, text, k=1):\n",
    "        return [ p[0] for p in sorted(self.scored_sentences(text), key=lambda p: p[1], reverse=True)[:k] ]\n",
    "\n",
    "\n",
    "class USETextRank(BaseSummarizer):\n",
    "    __embed__ = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return self.__embed__(sentences)\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "class TFIDFTextRank(BaseSummarizer):\n",
    "    __vectorizer__ = TfidfVectorizer()\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return tf.constant(self.__vectorizer__.fit_transform(sentences).todense())\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "class BERTFTextRank(BaseSummarizer):\n",
    "      #__tokenizer__ = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      #__embed__ = hub.Module(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", trainable=False)\n",
    "\n",
    "    def __sim_mat__(self, sentances: List[str]) -> tf.Tensor:\n",
    "        pairs = []\n",
    "        for s1 in sentances:\n",
    "            for s2 in sentances:\n",
    "                pairs.append((s1, s2))\n",
    "\n",
    "        input_ids, segment_ids, input_mask = tokenizer.batch_encode_plus(pairs, max_length=256, return_attention_mask = True)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "\n",
    "        bert_inputs = dict(\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          segment_ids=segment_ids)\n",
    "        sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
    "        ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "        return list(zip(sents, ranks.numpy()))\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "summarizerUSE = USETextRank()\n",
    "summarizerTFIDF = TFIDFTextRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "2JPVBlfumRwe",
    "outputId": "e82cfd73-c0c9-48fc-a4a5-8588927fcfe4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: cnn_dailymail/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset cnn_dailymail (/home/vad/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from /home/vad/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds_test = tfds.load(name=\"cnn_dailymail\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_BOHGvJy44R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/vad/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /cephfs/home/vad/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MAX_SEQ_LENGTH = 256\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_subnet = hub.KerasLayer(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", \n",
    "                      signature=\"tokens\", output_key=\"pooled_output\", trainable=True)\n",
    "\n",
    "def embedding4pair(s1: List[str], s2: List[str]) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    input_ids, segment_ids, input_mask = tokenizer.batch_encode_plus(pairs, \n",
    "                                  max_length=MAX_SEQ_LENGTH, return_attention_mask = True)\n",
    "    return input_ids, segment_ids, input_mask\n",
    "\n",
    "\n",
    "def create_ruler() -> tf.keras.Model:\n",
    "    i_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n",
    "    i_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=tf.int32)\n",
    "    i_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_inputs = {\"input_ids\": i_id, \"input_mask\": i_mask, \"segment_ids\": i_segment}\n",
    "  \n",
    "    embedding = bert_subnet(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, input_shape=(768,), activation='relu')(embedding)\n",
    "    d = tf.keras.layers.Dense(1, input_shape=(256,))(dense)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=bert_inputs, outputs=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "G-QtIs37IvMg",
    "outputId": "29153f72-1eb2-448d-ab22-d18cb176a694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Load pre-computed datasetinfo (eg: splits) from bucket.\n",
      "INFO:absl:Loading info from GCS for multi_nli/plain_text/1.0.0\n",
      "INFO:absl:Generating dataset multi_nli (/home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset multi_nli/plain_text/1.0.0 (download: 216.34 MiB, generated: Unknown size, total: 216.34 MiB) to /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395edee8a1c49bbbce27e5276c54ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c0777d83c74e64bda1748b7baaba02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2b681249364c4bad23051d4a762793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading http://storage.googleapis.com/tfds-data/downloads/multi_nli/multinli_1.0.zip into /home/vad/tensorflow_datasets/downloads/tfds-data_downloads_multi_nli_multinli_1.0HMUsk5OVZAJ-rEiUmzbrHqIXnZ_lNC_BY3bkXFsAYtY.zip.tmp.7fd8713105144addbf05102921940b1a...\n",
      "INFO:absl:Generating split train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0790037a894d46d392ae09b71fade131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=392702.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-train.tfrecord. Shard lengths: [392702]\n",
      "INFO:absl:Generating split validation_matched\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-validation_matched.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935c20669e8d4092a175fc83e3b912e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9815.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-validation_matched.tfrecord. Shard lengths: [9815]\n",
      "INFO:absl:Generating split validation_mismatched\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-validation_mismatched.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a75788bc9427ea9fdd76009fce9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9832.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0.incomplete0EB9KW/multi_nli-validation_mismatched.tfrecord. Shard lengths: [9832]\n",
      "INFO:absl:Skipping computing stats for mode ComputeStatsMode.AUTO.\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation_matched, from /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (/home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset multi_nli downloaded and prepared to /home/vad/tensorflow_datasets/multi_nli/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nli_validation = tfds.load(name=\"multi_nli\", split='validation_matched')\n",
    "nli_train = tfds.load(name=\"multi_nli\", split='train')\n",
    "\n",
    "def process_dataset(ds):\n",
    "    premises = []\n",
    "    hypothesis = []\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "    for x in ds:\n",
    "        p = x['premise'].numpy().decode('utf8')\n",
    "        l = x['label'].numpy()\n",
    "        h = x['hypothesis'].numpy().decode('utf8')\n",
    "        r = tokenizer.encode_plus(\n",
    "          pad_to_max_length='right',\n",
    "          text=p,\n",
    "          text_pair=h,\n",
    "          max_length=MAX_SEQ_LENGTH)\n",
    "  \n",
    "        input_ids.append(r['input_ids'])\n",
    "        input_mask.append(r['attention_mask'])\n",
    "        segment_ids.append(r['token_type_ids'])\n",
    "        if l == 1:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "        premises.append(p)\n",
    "        hypothesis.append(h)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    input_mask = np.array(input_mask, dtype=np.int32)\n",
    "    segment_ids = np.array(segment_ids, dtype=np.int32)\n",
    "\n",
    "    labels = np.array(labels, dtype=np.float16)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((input_ids, input_mask, segment_ids), labels))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "ds_nli_train = process_dataset(nli_train)\n",
    "ds_nli_valid = process_dataset(nli_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "gy8-FvD1eGof",
    "outputId": "6529029a-5559-44f8-8469-ceeedfcca742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 12 steps, validate for 1 steps\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "model = create_ruler()\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                            save_weights_only=True,\n",
    "                                            verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    ds_nli_train.shuffle(32868).batch(32868), \n",
    "    validation_data=ds_nli_valid.batch(32868), \n",
    "    callbacks=[es_cb, cp_cb], \n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRRBV9iszaMs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XQdA0YigClMK",
    "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
       "  array([  101, 24625,   117, 10817,   117, 16938,   112,   188, 13028,\n",
       "         21852, 13028,   112, 10323, 10590, 13507, 10741, 10142, 23457,\n",
       "           136,   102, 10357, 12482, 10108, 16624, 39784, 10662, 11223,\n",
       "         12153, 10106, 10105, 56538, 10155, 10226, 11795, 10108, 21997,\n",
       "           119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
       "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>),\n",
       " <tf.Tensor: shape=(), dtype=float16, numpy=1.0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds_nli_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdAW-Hrmr_ix"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uherj5PCCR-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "txts = []\n",
    "references = []\n",
    "for example in tqdm(ds_valid, total=len(list(ds_valid))):\n",
    "  references.append(example['highlights'].numpy().decode(\"utf-8\").split('\\n'))\n",
    "  txts.append(example['article'].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BekUXV9JADn-"
   },
   "outputs": [],
   "source": [
    "print('use', summarizerUSE.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7wNS9tOPr9M"
   },
   "outputs": [],
   "source": [
    "print('tfidf', summarizerTFIDF.bleu(references, txts))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
