{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "experiment.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAWVFWAYmRwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f23aa268-60f9-497e-bf0e-05f6d60ab9b9"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "!pip3 -q install tensorflow-datasets==2.1.0 tensorflow-text==2.1.1 transformers tensorflow-addons\n",
        "#!ulimit -n 1024"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZihwdUVsfUjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgrlRy3lmRwl",
        "colab_type": "code",
        "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from typing import List, Tuple\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class BaseSummarizer(object):\n",
        "    ROUND_DIGITS = 5\n",
        "\n",
        "    def __text2sentences__(self, text: str) -> List[str]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
        "        normalize = tf.math.l2_normalize(vec, 1)\n",
        "        cosine = tf.linalg.matmul(normalize, normalize, transpose_b=True)\n",
        "        rounded = tf.math.round(cosine * 10 ** BaseSummarizer.ROUND_DIGITS) / 10 ** BaseSummarizer.ROUND_DIGITS\n",
        "        return rounded\n",
        "\n",
        "    @staticmethod\n",
        "    def __ranks__(sent_sim_mat: tf.Tensor) -> tf.Tensor:\n",
        "        eig_val, eig_vec = tf.linalg.eigh(sent_sim_mat)\n",
        "        best_vector_idx = tf.math.argmax(eig_val)\n",
        "        return eig_vec[best_vector_idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def __z_score__(vec: tf.Tensor) -> tf.Tensor:\n",
        "       return (vec - tf.math.reduce_min(vec)) / (tf.math.reduce_max(vec) - tf.math.reduce_min(vec))\n",
        "\n",
        "    def bleu(self, references: List[List[str]], texts: List[str]):\n",
        "      score = 0.\n",
        "      smoothie = SmoothingFunction().method1\n",
        "\n",
        "      for refs, txt in zip(references, texts):\n",
        "        hyp = self.the_most_important(txt, k=1)[0]\n",
        "        score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
        "\n",
        "      score /= len(references)\n",
        "      return score\n",
        "\n",
        "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
        "        sents = self.__text2sentences__(text)\n",
        "        if not sents:\n",
        "            return []\n",
        "        sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
        "        ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
        "        return list(zip(sents, ranks.numpy()))\n",
        "\n",
        "    def the_most_important(self, text, k=1):\n",
        "        return [ p[0] for p in sorted(self.scored_sentences(text), key=lambda p: p[1], reverse=True)[:k] ]\n",
        "\n",
        "\n",
        "class USETextRank(BaseSummarizer):\n",
        "    __embed__ = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
        "        return self.__embed__(sentences)\n",
        "\n",
        "    def __text2sentences__(self, text: str) -> List[str]:\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "\n",
        "class TFIDFTextRank(BaseSummarizer):\n",
        "    __vectorizer__ = TfidfVectorizer()\n",
        "\n",
        "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
        "        return tf.constant(self.__vectorizer__.fit_transform(sentences).todense())\n",
        "\n",
        "    def __text2sentences__(self, text: str) -> List[str]:\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "  \n",
        "class BERTFTextRank(BaseSummarizer):\n",
        "  #__tokenizer__ = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  #__embed__ = hub.Module(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", trainable=False)\n",
        "\n",
        "  def __sim_mat__(self, sentances: List[str]) -> tf.Tensor:\n",
        "    pairs = []\n",
        "    for s1 in sentances:\n",
        "      for s2 in sentances:\n",
        "        pairs.append((s1, s2))\n",
        "\n",
        "    input_ids, segment_ids, input_mask = tokenizer.batch_encode_plus(pairs, max_length=256, return_attention_mask = True)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
        "    sents = self.__text2sentences__(text)\n",
        "    if not sents:\n",
        "      return []\n",
        "\n",
        "    bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "    sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
        "    ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
        "    return list(zip(sents, ranks.numpy()))\n",
        "\n",
        "  def __text2sentences__(self, text: str) -> List[str]:\n",
        "      return sent_tokenize(text)\n",
        "\n",
        "\n",
        "summarizerUSE = USETextRank()\n",
        "summarizerTFIDF = TFIDFTextRank()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JPVBlfumRwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e82cfd73-c0c9-48fc-a4a5-8588927fcfe4"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "ds_test = tfds.load(name=\"cnn_dailymail\", split='validation')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset cnn_dailymail (/root/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split validation, from /root/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BOHGvJy44R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Tuple\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "MAX_SEQ_LENGTH = 256\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "bert_subnet = hub.KerasLayer(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", \n",
        "                      signature=\"tokens\", output_key=\"pooled_output\", trainable=True)\n",
        "\n",
        "def embedding4pair(s1: List[str], s2: List[str]) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  input_ids, segment_ids, input_mask = tokenizer.batch_encode_plus(pairs, \n",
        "                                  max_length=MAX_SEQ_LENGTH, return_attention_mask = True)\n",
        "  return input_ids, segment_ids, input_mask\n",
        "\n",
        "\n",
        "def create_ruler() -> tf.keras.Model:\n",
        "  i_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n",
        "  i_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=tf.int32)\n",
        "  i_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=tf.int32)\n",
        "\n",
        "  bert_inputs = {\"input_ids\": i_id, \"input_mask\": i_mask, \"segment_ids\": i_segment}\n",
        "  \n",
        "  embedding = bert_subnet(bert_inputs)\n",
        "  dense = tf.keras.layers.Dense(256, input_shape=(768,), activation='relu')(embedding)\n",
        "  d = tf.keras.layers.Dense(1, input_shape=(256,))(dense)\n",
        "\n",
        "  return tf.keras.models.Model(inputs=bert_inputs, outputs=d)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-QtIs37IvMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "29153f72-1eb2-448d-ab22-d18cb176a694"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "nli_validation = tfds.load(name=\"multi_nli\", split='validation_matched')\n",
        "nli_train = tfds.load(name=\"multi_nli\", split='train')\n",
        "\n",
        "def process_dataset(ds):\n",
        "  premises = []\n",
        "  hypothesis = []\n",
        "  input_ids = []\n",
        "  input_mask = []\n",
        "  segment_ids = []\n",
        "  labels = []\n",
        "  for x in ds:\n",
        "    p = x['premise'].numpy().decode('utf8')\n",
        "    l = x['label'].numpy()\n",
        "    h = x['hypothesis'].numpy().decode('utf8')\n",
        "    r = tokenizer.encode_plus(\n",
        "      pad_to_max_length='right',\n",
        "      text=p,\n",
        "      text_pair=h,\n",
        "      max_length=MAX_SEQ_LENGTH)\n",
        "  \n",
        "    input_ids.append(r['input_ids'])\n",
        "    input_mask.append(r['attention_mask'])\n",
        "    segment_ids.append(r['token_type_ids'])\n",
        "    if l == 1:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "\n",
        "    premises.append(p)\n",
        "    hypothesis.append(h)\n",
        "\n",
        "  input_ids = np.array(input_ids, dtype=np.int32)\n",
        "  input_mask = np.array(input_mask, dtype=np.int32)\n",
        "  segment_ids = np.array(segment_ids, dtype=np.int32)\n",
        "\n",
        "  labels = np.array(labels, dtype=np.float16)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(((input_ids, input_mask, segment_ids), labels))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "ds_nli_train = process_dataset(nli_train)\n",
        "ds_nli_valid = process_dataset(nli_validation)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset multi_nli (/root/tensorflow_datasets/multi_nli/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split validation_matched, from /root/tensorflow_datasets/multi_nli/plain_text/1.0.0\n",
            "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset multi_nli (/root/tensorflow_datasets/multi_nli/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train, from /root/tensorflow_datasets/multi_nli/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWgh9t6p9wzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WFEvW3mpeU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy8-FvD1eGof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6529029a-5559-44f8-8469-ceeedfcca742"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "model = create_ruler()\n",
        "\n",
        "model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.losses.MeanSquaredError()]\n",
        ")\n",
        "\n",
        "model.fit(ds_nli_train.shuffle(1024).batch(512), validation_data=ds_nli_valid.batch(512), epochs=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 767 steps, validate for 20 steps\n",
            "Epoch 1/15\n",
            "  2/767 [..............................] - ETA: 97:07:46 - loss: 44.5007 - mean_squared_error: 44.5007"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRRBV9iszaMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQdA0YigClMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
      },
      "source": [
        "next(iter(ds_nli_valid))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((<tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
              "  array([  101, 24625,   117, 10817,   117, 16938,   112,   188, 13028,\n",
              "         21852, 13028,   112, 10323, 10590, 13507, 10741, 10142, 23457,\n",
              "           136,   102, 10357, 12482, 10108, 16624, 39784, 10662, 11223,\n",
              "         12153, 10106, 10105, 56538, 10155, 10226, 11795, 10108, 21997,\n",
              "           119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
              "  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
              "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>),\n",
              " <tf.Tensor: shape=(), dtype=float16, numpy=1.0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdAW-Hrmr_ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uherj5PCCR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "txts = []\n",
        "references = []\n",
        "for example in tqdm(ds_valid, total=len(list(ds_valid))):\n",
        "  references.append(example['highlights'].numpy().decode(\"utf-8\").split('\\n'))\n",
        "  txts.append(example['article'].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BekUXV9JADn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('use', summarizerUSE.bleu(references, txts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7wNS9tOPr9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('tfidf', summarizerTFIDF.bleu(references, txts))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}