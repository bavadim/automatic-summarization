{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AAWVFWAYmRwR",
    "outputId": "f23aa268-60f9-497e-bf0e-05f6d60ab9b9"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "#%load_ext tensorboard\n",
    "#!pip3 -q install -U tensorflow==2.1.0 tensorflow-gpu==2.1.0 tensorflow-datasets==2.1.0 tensorflow-text==2.1.1 tensorflow-hub==0.7.0 nltk sklearn transformers tensorflow-addons \n",
    "!pip3 -q install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using datasets to cache modules.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow_datasets as tfds\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer\n",
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = 'datasets'\n",
    "\n",
    "\n",
    "class BaseSummarizer(object):\n",
    "    ROUND_DIGITS = 5\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
    "        normalize = tf.math.l2_normalize(vec, 1)\n",
    "        cosine = tf.linalg.matmul(normalize, normalize, transpose_b=True)\n",
    "        rounded = tf.math.round(cosine * 10 ** BaseSummarizer.ROUND_DIGITS) / 10 ** BaseSummarizer.ROUND_DIGITS\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def __ranks__(sent_sim_mat: tf.Tensor) -> tf.Tensor:\n",
    "        eig_val, eig_vec = tf.linalg.eigh(sent_sim_mat)\n",
    "        best_vector_idx = tf.math.argmax(eig_val)\n",
    "        return eig_vec[best_vector_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def __z_score__(vec: tf.Tensor) -> tf.Tensor:\n",
    "        return (vec - tf.math.reduce_min(vec)) / (tf.math.reduce_max(vec) - tf.math.reduce_min(vec))\n",
    "\n",
    "    def bleu(self, references: List[List[str]], texts: List[str]):\n",
    "        score = 0.\n",
    "        smoothie = SmoothingFunction().method1\n",
    "\n",
    "        for refs, txt in tqdm(zip(references, texts), total=len(references)):\n",
    "            hyp = self.the_most_important(txt, k=1)[0]\n",
    "            score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
    "\n",
    "        score /= len(references)\n",
    "        return score\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "        sim_mat = self.__sim_mat__(self.__embeddings__(sents))\n",
    "        ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "        return list(zip(sents, ranks.numpy()))\n",
    "\n",
    "    def the_most_important(self, text, k=1):\n",
    "        return [ p[0] for p in sorted(self.scored_sentences(text), key=lambda p: p[1], reverse=True)[:k] ]\n",
    "\n",
    "\n",
    "class USETextRank(BaseSummarizer):\n",
    "    __embed__ = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return self.__embed__(sentences)\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "class TFIDFTextRank(BaseSummarizer):\n",
    "    __vectorizer__ = TfidfVectorizer()\n",
    "\n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return tf.constant(self.__vectorizer__.fit_transform(sentences).todense())\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "summarizerUSE = USETextRank()\n",
    "summarizerTFIDF = TFIDFTextRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def create_ruler() -> tf.keras.Model:\n",
    "    \n",
    "    i_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n",
    "    i_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=tf.int32)\n",
    "    i_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=tf.int32)\n",
    "                                \n",
    "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\", trainable=True)\n",
    "        \n",
    "    pooled_output, _ = bert_layer([i_id, i_mask, i_segment])\n",
    "  \n",
    "    d = tf.keras.layers.Dense(3)(pooled_output)\n",
    "\n",
    "    return tf.keras.models.Model(inputs={\"input_ids\": i_id, \"input_mask\": i_mask, \"segment_ids\": i_segment}, outputs=d)\n",
    "\n",
    "def convert_datasets():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    \n",
    "    nli_validation = tfds.load(name=\"multi_nli\", split='validation_matched', data_dir='datasets')\n",
    "    nli_train = tfds.load(name=\"multi_nli\", split='train', data_dir='datasets')\n",
    "    \n",
    "    def write(ds, outf):\n",
    "        def embedding4pair(s1: str, s2: str) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "            r = tokenizer.encode_plus(\n",
    "                  pad_to_max_length='right',\n",
    "                  text=s1,\n",
    "                  text_pair=s2,\n",
    "                  max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "            return { \n",
    "                'input_ids': r['input_ids'], \n",
    "                'input_mask': r['attention_mask'], \n",
    "                'segment_ids': r['token_type_ids']\n",
    "            }\n",
    "\n",
    "        \n",
    "        with open(outf, \"w\") as write_file:\n",
    "            for x in ds:\n",
    "                p = x['premise'].numpy().decode('utf8')\n",
    "                l = x['label'].numpy()\n",
    "                h = x['hypothesis'].numpy().decode('utf8')\n",
    "                \n",
    "                r = embedding4pair(p, h)\n",
    "                r['label'] = int(l)\n",
    "                r['s1'] = h\n",
    "                r['s2'] = p\n",
    "                \n",
    "                json.dump(r, write_file)\n",
    "                write_file.write(\"\\n\")\n",
    "        \n",
    "    write(nli_train, 'nli.train.jsonl')\n",
    "    write(nli_validation, 'nli.valid.jsonl')\n",
    "\n",
    "#convert_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('nli.train.jsonl', lines=True)\n",
    "valid = pd.read_json('nli.valid.jsonl', lines=True)\n",
    "\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices(((np.stack(train['input_ids'].map(lambda x: np.array(x, dtype=np.int32)).values), \n",
    "                                                  np.stack(train['input_mask'].map(lambda x: np.array(x, dtype=np.int32)).values), \n",
    "                                                  np.stack(train['segment_ids'].map(lambda x: np.array(x, dtype=np.int32)).values)), \n",
    "                                                 train['label'].values))\n",
    "vl_dataset = tf.data.Dataset.from_tensor_slices(((np.stack(valid['input_ids'].map(lambda x: np.array(x, dtype=np.int32)).values), \n",
    "                                                  np.stack(valid['input_mask'].map(lambda x: np.array(x, dtype=np.int32)).values), \n",
    "                                                  np.stack(valid['segment_ids'].map(lambda x: np.array(x, dtype=np.int32)).values)), \n",
    "                                                 valid['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 14864, 90086, 26900, 10686, 10124, 11847...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The narthex and the main entrance are located ...</td>\n",
       "      <td>Its narthex is set at an angle to the main ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 11696, 34420, 112, 188, 12888, 10105, 15...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>They were put in a museum after they were exca...</td>\n",
       "      <td>They didn't see the light of day until their e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 37025, 136, 11723, 10134, 10192, 15453, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>He just looked at me and said, Well, what is it?</td>\n",
       "      <td>Well? There was no change of expression in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 10117, 20826, 10124, 10105, 22013, 10108...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The winner has the fastest actions.</td>\n",
       "      <td>The winner is the master of the cleverest ploy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 10117, 160, 14383, 64424, 10165, 30598, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The area is entirely wild, with no signs of ci...</td>\n",
       "      <td>The Whinlatter Pass Visitor Centre is a hub fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392697</th>\n",
       "      <td>[101, 10167, 18638, 117, 12277, 13028, 16938, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>The only way to travel to the tree-shaded aven...</td>\n",
       "      <td>In fact, if you don't feel like taking the bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392698</th>\n",
       "      <td>[101, 12865, 72894, 74755, 45788, 20442, 52253...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>We need your help, we can't take care of ourse...</td>\n",
       "      <td>We appreciate your desire to help but we can t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392699</th>\n",
       "      <td>[101, 19687, 46739, 117, 169, 27914, 10111, 17...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>The concord coalition has open positions.</td>\n",
       "      <td>Neil Howe, a historian and economist, is a sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392700</th>\n",
       "      <td>[101, 10380, 10189, 112, 187, 11023, 12257, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>That's one of the ways we do it, although ther...</td>\n",
       "      <td>so that's yeah that's one of the that's one of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392701</th>\n",
       "      <td>[101, 14962, 13172, 10108, 10105, 11356, 10894...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>There was a question of how much of the world ...</td>\n",
       "      <td>How much of the world would Milosevic sacrific...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392702 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input_ids  \\\n",
       "0       [101, 14864, 90086, 26900, 10686, 10124, 11847...   \n",
       "1       [101, 11696, 34420, 112, 188, 12888, 10105, 15...   \n",
       "2       [101, 37025, 136, 11723, 10134, 10192, 15453, ...   \n",
       "3       [101, 10117, 20826, 10124, 10105, 22013, 10108...   \n",
       "4       [101, 10117, 160, 14383, 64424, 10165, 30598, ...   \n",
       "...                                                   ...   \n",
       "392697  [101, 10167, 18638, 117, 12277, 13028, 16938, ...   \n",
       "392698  [101, 12865, 72894, 74755, 45788, 20442, 52253...   \n",
       "392699  [101, 19687, 46739, 117, 169, 27914, 10111, 17...   \n",
       "392700  [101, 10380, 10189, 112, 187, 11023, 12257, 10...   \n",
       "392701  [101, 14962, 13172, 10108, 10105, 11356, 10894...   \n",
       "\n",
       "                                               input_mask  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "392697  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392698  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392699  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392700  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392701  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              segment_ids  label  \\\n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2   \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1   \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0   \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2   \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2   \n",
       "...                                                   ...    ...   \n",
       "392697  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2   \n",
       "392698  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2   \n",
       "392699  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1   \n",
       "392700  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0   \n",
       "392701  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0   \n",
       "\n",
       "                                                       s1  \\\n",
       "0       The narthex and the main entrance are located ...   \n",
       "1       They were put in a museum after they were exca...   \n",
       "2        He just looked at me and said, Well, what is it?   \n",
       "3                     The winner has the fastest actions.   \n",
       "4       The area is entirely wild, with no signs of ci...   \n",
       "...                                                   ...   \n",
       "392697  The only way to travel to the tree-shaded aven...   \n",
       "392698  We need your help, we can't take care of ourse...   \n",
       "392699          The concord coalition has open positions.   \n",
       "392700  That's one of the ways we do it, although ther...   \n",
       "392701  There was a question of how much of the world ...   \n",
       "\n",
       "                                                       s2  \n",
       "0       Its narthex is set at an angle to the main ent...  \n",
       "1       They didn't see the light of day until their e...  \n",
       "2       Well? There was no change of expression in the...  \n",
       "3       The winner is the master of the cleverest ploy...  \n",
       "4       The Whinlatter Pass Visitor Centre is a hub fo...  \n",
       "...                                                   ...  \n",
       "392697  In fact, if you don't feel like taking the bus...  \n",
       "392698  We appreciate your desire to help but we can t...  \n",
       "392699  Neil Howe, a historian and economist, is a sen...  \n",
       "392700  so that's yeah that's one of the that's one of...  \n",
       "392701  How much of the world would Milosevic sacrific...  \n",
       "\n",
       "[392702 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "model = create_ruler()\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[ 'sparse_categorical_accuracy' ]\n",
    ")\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                            save_weights_only=True,\n",
    "                                            verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            2307        keras_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 177,855,748\n",
      "Trainable params: 177,855,747\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 24544 steps, validate for 614 steps\n",
      "Epoch 1/3\n",
      "  317/24544 [..............................] - ETA: 3:37:18 - loss: 1.2864 - sparse_categorical_accuracy: 0.3314"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    tr_dataset.shuffle(BATCH_SIZE*512*32).batch(BATCH_SIZE), \n",
    "    validation_data=vl_dataset.shuffle(BATCH_SIZE*512*32).batch(BATCH_SIZE), \n",
    "    callbacks=[es_cb, cp_cb], \n",
    "    epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JgrlRy3lmRwl",
    "outputId": "2a0a4681-ac37-4286-8bb2-97892747e91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "MAX_SEQ_LENGTH = 256\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def read_csv():\n",
    "    \n",
    "    labels = []\n",
    "    def reader():\n",
    "        with open('cnn_dailymail.csv', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            input_ids_batch = []\n",
    "            input_mask_batch = []\n",
    "            segment_ids_batch = []\n",
    "            \n",
    "            for i, doc in enumerate(reader):\n",
    "                sents = sent_tokenize(doc['article'])\n",
    "                references = doc['highlights'].split('\\n')\n",
    "                for s1 in sents:\n",
    "                    for s2 in sents:\n",
    "                        r = tokenizer.encode_plus(pad_to_max_length='right', text=s1, text_pair=s2, max_length=MAX_SEQ_LENGTH)\n",
    "                        input_ids_batch.append(r['input_ids'])\n",
    "                        input_mask_batch.append(r['input_mask'])\n",
    "                        segment_ids_batch.append(r['segment_ids'])\n",
    "\n",
    "                        labels.append(i)\n",
    "\n",
    "                        if len(input_ids_batch) < BATCH_SIZE:\n",
    "                            continue\n",
    "                        else:\n",
    "                            yield ({ \n",
    "                                'input_ids': np.array(input_ids_batch, dtype=np.int32), \n",
    "                                'input_masks': np.array(input_mask_batch, dtype=np.int32), \n",
    "                                'segment_ids': np.array(segment_ids_batch, dtype=np.int32)})\n",
    "                            input_ids_batch = []\n",
    "                            input_mask_batch = []\n",
    "                            segment_ids_batch = []\n",
    "\n",
    "    return reader(), labels                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `batch_size` argument must not be specified for the given input type. Received input: <generator object read_csv at 0x7f8dd155c450>, batch_size: 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-57d733e5010c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     batch_size = model._validate_or_infer_batch_size(\n\u001b[0;32m--> 407\u001b[0;31m         batch_size, steps, x)\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_distribution_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     batch_size, steps = dist_utils.process_batch_and_step_size(\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_or_infer_batch_size\u001b[0;34m(self, batch_size, steps, x)\u001b[0m\n\u001b[1;32m   1754\u001b[0m             \u001b[0;34m'The `batch_size` argument must not be specified for the given '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m             'input type. Received input: {}, batch_size: {}'.format(\n\u001b[0;32m-> 1756\u001b[0;31m                 x, batch_size))\n\u001b[0m\u001b[1;32m   1757\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The `batch_size` argument must not be specified for the given input type. Received input: <generator object read_csv at 0x7f8dd155c450>, batch_size: 512"
     ]
    }
   ],
   "source": [
    "ds_p = ds.batch(512)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model('simple', compile=False )\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "\n",
    "X, l = read_csv()\n",
    "y = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-7361b8f276b7>\", line 47, in <module>\n",
      "    flat_matrix = encode_pairs(ss1, ss2).numpy()\n",
      "  File \"<ipython-input-2-7361b8f276b7>\", line 28, in encode_pairs\n",
      "    r = tokenizer.encode_plus(pad_to_max_length='right', text=s1, text_pair=s2, max_length=MAX_SEQ_LENGTH)\n",
      "NameError: name 'tokenizer' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/mnt/d/work/automatic-summarization/env/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = tf.keras.models.load_model('simple', compile=False )\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "def sentance_pairs(docs: List[str]) -> Tuple[List[str], List[str], List[int]]:\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    ids = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        sents = sent_tokenize(doc)\n",
    "        for s1 in sents:\n",
    "            for s2 in sents:\n",
    "                res1.append(s1)\n",
    "                res2.append(s2)\n",
    "                ids.append(i)\n",
    "    return (res1, res2, ids)\n",
    "\n",
    "def encode_pairs(ss1: List[str], ss2: List[str]) -> tf.Tensor:\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    for s1, s2 in zip(ss1, ss2):\n",
    "        r = tokenizer.encode_plus(pad_to_max_length='right', text=s1, text_pair=s2, max_length=MAX_SEQ_LENGTH)\n",
    "        input_ids.append(r['input_ids'])\n",
    "        input_mask.append(r['attention_mask'])\n",
    "        segment_ids.append(r['token_type_ids'])\n",
    "        \n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    input_mask = np.array(input_mask, dtype=np.int32)\n",
    "    segment_ids = np.array(segment_ids, dtype=np.int32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_ids, input_mask, segment_ids))\n",
    "    \n",
    "    return model.predict(dataset.batch(512))\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('cnn_dailymail.csv')\n",
    "references = df_test['highlights'].map(lambda x: np.array(x.split('\\n'), dtype=str)).values\n",
    "txts = df_test['article'].values\n",
    "\n",
    "ss1, ss2, ids = sentance_pairs(txts)\n",
    "flat_matrix = encode_pairs(ss1, ss2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_BOHGvJy44R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation_matched, from datasets/multi_nli/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from datasets/multi_nli/plain_text/1.0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2d25396fd0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mnli_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi_nli\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation_matched'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mnli_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi_nli\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mds_nli_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnli_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mds_nli_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnli_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fe7e2a121bc9>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           max_length=MAX_SEQ_LENGTH)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, **kwargs)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             )\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    804\u001b[0m                     (\n\u001b[1;32m    805\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m                     )\n\u001b[1;32m    808\u001b[0m                 )\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    804\u001b[0m                     (\n\u001b[1;32m    805\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_added_tokens_encoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m                     )\n\u001b[1;32m    808\u001b[0m                 )\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_is_punctuation\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m\"\"\"Checks whether `chars` is a punctuation character.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0;31m# We treat all non-letter/number ASCII as punctuation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "MAX_SEQ_LENGTH = 256\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_subnet = hub.KerasLayer(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", \n",
    "                      signature=\"tokens\", output_key=\"pooled_output\", trainable=True)\n",
    "\n",
    "def embedding4pair(s1: str, s2: str) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "    r = tokenizer.encode_plus(\n",
    "          pad_to_max_length='right',\n",
    "          text=s1,\n",
    "          text_pair=s2,\n",
    "          max_length=MAX_SEQ_LENGTH)\n",
    "    \n",
    "    return tf.constant(r['input_ids']), tf.constant(r['attention_mask']), tf.constant(r['token_type_ids'])\n",
    "\n",
    "\n",
    "def create_ruler() -> tf.keras.Model:\n",
    "    i_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\", dtype=tf.int32)\n",
    "    i_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\", dtype=tf.int32)\n",
    "    i_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_inputs = {\"input_ids\": i_id, \"input_mask\": i_mask, \"segment_ids\": i_segment}\n",
    "  \n",
    "    embedding = bert_subnet(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, input_shape=(768,), activation='relu')(embedding)\n",
    "    d = tf.keras.layers.Dense(1, input_shape=(256,))(dense)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=bert_inputs, outputs=d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "G-QtIs37IvMg",
    "outputId": "29153f72-1eb2-448d-ab22-d18cb176a694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation_matched, from datasets/multi_nli/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: multi_nli/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset multi_nli (datasets/multi_nli/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from datasets/multi_nli/plain_text/1.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "nli_validation = tfds.load(name=\"multi_nli\", split='validation_matched', data_dir='datasets')\n",
    "nli_train = tfds.load(name=\"multi_nli\", split='train', data_dir='datasets')\n",
    "\n",
    "def process_dataset(ds):\n",
    "    premises = []\n",
    "    hypothesis = []\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    segment_ids = []\n",
    "    labels = []\n",
    "    for x in ds:\n",
    "        p = x['premise'].numpy().decode('utf8')\n",
    "        l = x['label'].numpy()\n",
    "        h = x['hypothesis'].numpy().decode('utf8')\n",
    "        r = tokenizer.encode_plus(\n",
    "          pad_to_max_length='right',\n",
    "          text=p,\n",
    "          text_pair=h,\n",
    "          max_length=MAX_SEQ_LENGTH)\n",
    "  \n",
    "        input_ids.append(r['input_ids'])\n",
    "        input_mask.append(r['attention_mask'])\n",
    "        segment_ids.append(r['token_type_ids'])\n",
    "        if l == 1:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "        premises.append(p)\n",
    "        hypothesis.append(h)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    input_mask = np.array(input_mask, dtype=np.int32)\n",
    "    segment_ids = np.array(segment_ids, dtype=np.int32)\n",
    "\n",
    "    labels = np.array(labels, dtype=np.float16)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((input_ids, input_mask, segment_ids), labels))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "ds_nli_train = process_dataset(nli_train)\n",
    "ds_nli_valid = process_dataset(nli_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "gy8-FvD1eGof",
    "outputId": "6529029a-5559-44f8-8469-ceeedfcca742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 767 steps, validate for 20 steps\n",
      "Epoch 1/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2209 - mean_squared_error: 0.2209 \n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4154s 5s/step - loss: 0.2209 - mean_squared_error: 0.2209 - val_loss: 0.2413 - val_mean_squared_error: 0.2413\n",
      "Epoch 2/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2120 - mean_squared_error: 0.2120 \n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4156s 5s/step - loss: 0.2120 - mean_squared_error: 0.2120 - val_loss: 0.1985 - val_mean_squared_error: 0.1985\n",
      "Epoch 3/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2091 - mean_squared_error: 0.2091 \n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4157s 5s/step - loss: 0.2091 - mean_squared_error: 0.2091 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 4/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2049 - mean_squared_error: 0.2049 \n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.2049 - mean_squared_error: 0.2049 - val_loss: 0.1929 - val_mean_squared_error: 0.1929\n",
      "Epoch 5/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2022 - mean_squared_error: 0.2022 \n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4158s 5s/step - loss: 0.2022 - mean_squared_error: 0.2022 - val_loss: 0.1920 - val_mean_squared_error: 0.1920\n",
      "Epoch 6/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2020 - mean_squared_error: 0.2020 \n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.2020 - mean_squared_error: 0.2020 - val_loss: 0.1916 - val_mean_squared_error: 0.1916\n",
      "Epoch 7/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.2009 - mean_squared_error: 0.2009 \n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4156s 5s/step - loss: 0.2009 - mean_squared_error: 0.2009 - val_loss: 0.1881 - val_mean_squared_error: 0.1881\n",
      "Epoch 8/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1996 - mean_squared_error: 0.1996 \n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4155s 5s/step - loss: 0.1996 - mean_squared_error: 0.1996 - val_loss: 0.1979 - val_mean_squared_error: 0.1979\n",
      "Epoch 9/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1970 - mean_squared_error: 0.1970 \n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "767/767 [==============================] - 4153s 5s/step - loss: 0.1970 - mean_squared_error: 0.1970 - val_loss: 0.1976 - val_mean_squared_error: 0.1976\n",
      "Epoch 11/15\n",
      "766/767 [============================>.] - ETA: 5s - loss: 0.1966 - mean_squared_error: 0.1966 "
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "model = create_ruler()\n",
    "\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                            save_weights_only=True,\n",
    "                                            verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    ds_nli_train.shuffle(32868).batch(512), \n",
    "    validation_data=ds_nli_valid.batch(512), \n",
    "    callbacks=[es_cb, cp_cb], \n",
    "    epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRRBV9iszaMs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XQdA0YigClMK",
    "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('cnn_dailymail.csv')\n",
    "references = df_test['highlights'].map(lambda x: np.array(x.split('\\n'), dtype=str)).values\n",
    "txts = df_test['article'].values\n",
    "\n",
    "model = tf.keras.models.load_model('simple', compile=False )\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "\n",
    "y = []\n",
    "ss = []\n",
    "\n",
    "for rfs, txt in tqdm(zip(references, txts), total=len(txts)):\n",
    "    sntns = sent_tokenize(txt)\n",
    "    y.append(rfs)\n",
    "    ss.append(sntns)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XQdA0YigClMK",
    "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py:677 map_fn\n        batch_size=None)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:526 standardize_input_data\n        standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:526 <listcomp>\n        standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:451 standardize_single_array\n        if (x.shape is not None and len(x.shape) == 1 and\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:822 __len__\n        raise ValueError(\"Cannot take the length of shape with unknown rank.\")\n\n    ValueError: Cannot take the length of shape with unknown rank.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d38d8b11f70a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    682\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1591\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3926\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py:677 map_fn\n        batch_size=None)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:526 standardize_input_data\n        standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:526 <listcomp>\n        standardize_single_array(x, shape) for (x, shape) in zip(data, shapes)\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py:451 standardize_single_array\n        if (x.shape is not None and len(x.shape) == 1 and\n    /mnt/d/work/automatic-summarization/env/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py:822 __len__\n        raise ValueError(\"Cannot take the length of shape with unknown rank.\")\n\n    ValueError: Cannot take the length of shape with unknown rank.\n"
     ]
    }
   ],
   "source": [
    "def proc():\n",
    "    i = []\n",
    "    a = []\n",
    "    m = []\n",
    "    for sntns in tqdm(ss, total=len(ss)):\n",
    "        for v1 in sntns:\n",
    "            for v2 in sntns:\n",
    "                r = embedding4pair(v1, v2)\n",
    "\n",
    "                i.append(r[0])\n",
    "                a.append(r[1])\n",
    "                m.append(r[2])\n",
    "                \n",
    "                yield (r[0], r[1], r[2])\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(proc, output_types=tf.float16).batch(512)\n",
    "p = model.predict(ds).numpy()\n",
    "\n",
    "offset = 0\n",
    "sim = []\n",
    "for s in tqdm(ss, total=len(ss)):\n",
    "    if offset >= len(i):\n",
    "        break\n",
    "\n",
    "    sentences_count = len(s) * len(s)\n",
    "    sim_mat = p[offset:(offset + sentences_count)].reshape((len(s), len(s)))\n",
    "    offset =+ sentences_count\n",
    "\n",
    "    sim.append(sim_mat)\n",
    "\n",
    "score = 0.\n",
    "smoothie = SmoothingFunction().method1\n",
    "for sents, sim_mat, refs in tqdm(zip(ss, sim, y), total=len(y)):\n",
    "    ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "    scored_sentences = list(zip(sents, ranks.numpy()))\n",
    "    the_most_important = [ p[0] for p in sorted(scored_sentences, key=lambda p: p[1], reverse=True)[:1] ]\n",
    "    hyp = the_most_important[0]\n",
    "    score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
    "\n",
    "score /= len(y)\n",
    "print('bert', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XQdA0YigClMK",
    "outputId": "0098de5e-4894-426c-865b-b261d07d0478"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('cnn_dailymail.csv')\n",
    "references = df_test['highlights'].map(lambda x: np.array(x.split('\\n'), dtype=str)).values\n",
    "txts = df_test['article'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdAW-Hrmr_ix"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13368/13368 [21:58<00:00, 10.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = tf.keras.models.load_model('simple', compile=False )\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.LAMB(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.losses.MeanSquaredError()]\n",
    ")\n",
    "        \n",
    "    \n",
    "class BERTTextRank(BaseSummarizer):\n",
    "    \n",
    "    def __embeddings__(self, sentences: List[str]) -> tf.Tensor:\n",
    "        return tf.constant(sentences, dtype=tf.string)\n",
    "    \n",
    "    def __sim_mat__(self, vec: tf.Tensor) -> tf.Tensor:\n",
    "        vecs = vec.numpy()[0:15]\n",
    "        \n",
    "        res = []\n",
    "        i = []\n",
    "        a = []\n",
    "        m = []\n",
    "         \n",
    "        for v1 in vecs:\n",
    "            for v2 in vecs:\n",
    "                r = embedding4pair(v1.decode('utf8'), v2.decode('utf8'))\n",
    "                i.append(r[0])\n",
    "                a.append(r[1])\n",
    "                m.append(r[2])\n",
    "        p = model.predict_on_batch((i, a, m)).numpy()\n",
    "        res = p.reshape((len(vecs), len(vecs)))\n",
    "                \n",
    "        return res\n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "ss = []\n",
    "for rfs, txt in tqdm(zip(references, txts), total=len(txts)):\n",
    "    sntns = sent_tokenize(txt)[0:15]\n",
    "    i = []\n",
    "    a = []\n",
    "    m = []\n",
    "    for v1 in sntns:\n",
    "        for v2 in sntns:\n",
    "            r = embedding4pair(v1, v2)\n",
    "            i.append(r[0])\n",
    "            a.append(r[1])\n",
    "            m.append(r[2])\n",
    "            \n",
    "    X.append((i, a, m))\n",
    "    y.append(rfs)\n",
    "    ss.append(sntns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3582/13368 [8:47:52<24:57:53,  9.18s/it]"
     ]
    }
   ],
   "source": [
    "sim = []\n",
    "for x, s in tqdm(zip(X, ss), total=len(X)):\n",
    "    p = model.predict_on_batch((x[0], x[1], x[2])).numpy()\n",
    "    sim.append(p.reshape((len(s), len(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.\n",
    "smoothie = SmoothingFunction().method1\n",
    "for sents, sim_mat, refs in zip(ss, sim, y):\n",
    "    ranks = BaseSummarizer.__z_score__(BaseSummarizer.__ranks__(sim_mat))\n",
    "    scored_sentences = list(zip(sents, ranks.numpy()))\n",
    "    the_most_important = [ p[0] for p in sorted(scored_sentences, key=lambda p: p[1], reverse=True)[:1] ]\n",
    "    hyp = the_most_important[0]\n",
    "    score += sentence_bleu([ nltk.word_tokenize(s) for s in refs ], nltk.word_tokenize(hyp), smoothing_function=smoothie)\n",
    "\n",
    "score /= len(y)\n",
    "print('bert', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BekUXV9JADn-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use 0.061298139955609794\n"
     ]
    }
   ],
   "source": [
    "print('use', summarizerUSE.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7wNS9tOPr9M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0.06028948961872044\n"
     ]
    }
   ],
   "source": [
    "print('tfidf', summarizerTFIDF.bleu(references, txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTFTextRank(BaseSummarizer):\n",
    "      #__tokenizer__ = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      #__embed__ = hub.Module(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\", trainable=False)\n",
    "\n",
    "    def scored_sentences(self, text: str) -> List[Tuple[str, float]]:\n",
    "        sents = self.__text2sentences__(text)\n",
    "        if not sents:\n",
    "            return []\n",
    "\n",
    "        \n",
    "\n",
    "    def __text2sentences__(self, text: str) -> List[str]:\n",
    "        return sent_tokenize(text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "experiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
